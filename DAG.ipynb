{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0299dbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "from airflow.utils.trigger_rule import TriggerRule\n",
    "from airflow.models import Variable\n",
    "from airflow.hooks.S3_hook import S3Hook\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import logging\n",
    "\n",
    "\n",
    "log = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee355ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f192f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_args = {\n",
    "    'start_date': datetime(2022, 4, 14),\n",
    "    'owner': 'Airflow',\n",
    "    'filestore_base': '/tmp/airflowtemp/',\n",
    "    'email_on_failure': False,\n",
    "    'email_on_retry': False,\n",
    "    'url_to_scrape': Variable.get(\"example_web_scraping_pipeline\", deserialize_json=True)['url_to_scrape'],\n",
    "    'aws_conn_id': \"aws_default\",\n",
    "    'bucket_name': Variable.get(\"example_web_scraping_pipeline\", deserialize_json=True)['bucket_name'],\n",
    "    's3_key': Variable.get(\"example_web_scraping_pipeline\", deserialize_json=True)['s3_key'],\n",
    "    'retries': 0,\n",
    "    'retry_delay': timedelta(minutes=5)\n",
    "}\n",
    "\n",
    "dag = DAG('DE166_web_scraping_pipeline',\n",
    "          description='a web scraping pipeline that save the outout to a csv file in S3',\n",
    "          schedule_interval='@weekly',\n",
    "          catchup=False,\n",
    "          default_args=default_args,\n",
    "          max_active_runs=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0931fffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def web_scraping_function(**kwargs):\n",
    "\n",
    "    # Import packages\n",
    "    import requests\n",
    "    from bs4 import BeautifulSoup\n",
    "    from urllib.parse import urljoin\n",
    "\n",
    "    # Specify url\n",
    "    url = kwargs['url_to_scrape']\n",
    "    base_url = urljoin(url, \"/\").rstrip(\"/\")\n",
    "\n",
    "    log.info('Going to scrape data from {0}'.format(url))\n",
    "\n",
    "    # Package the request, send the request and catch the response: r\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0 (X11; CrOS x86_64 12871.102.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.141 Safari/537.36\"}\n",
    "    html = requests.get(url, headers=headers,timeout=10)\n",
    "\n",
    "    # Extracts the response as html: html_doc\n",
    "\n",
    "\n",
    "    # create a BeautifulSoup object from the HTML: soup\n",
    "    soup = BeautifulSoup(html.text, 'lxml')\n",
    "\n",
    "    # Find all 'h2' tags (which define hyperlinks): h_tags\n",
    "    tableContent = soup.find_all(\"td\", {\"class\": \"govuk-table__cell\"})\n",
    "\n",
    "    companyName = []\n",
    "    companyNumber = []\n",
    "    status = []\n",
    "    link = []\n",
    "    log.info('Going to scrape data from website')\n",
    "\n",
    "    # Iterate over all the h2 tags found to extract the link and \n",
    "    nextpage=soup.find_all('div', {'class': \"pagination\"})\n",
    "    nexpageUrl = nextpage[0].find('a',{\"id\":\"nextLink\"})\n",
    "    urltext= \"https://find-and-update.company-information.service.gov.uk/alphabetical-search/\"+str( nexpageUrl.get('href'))\n",
    "    print(urltext)\n",
    "\n",
    "    for i in range(int(len(tableContent)/3)):\n",
    "        indexNumber = i*3\n",
    "        companyName.append(tableContent[indexNumber].find(text=True)) \n",
    "        number = tableContent[indexNumber+1].find(text=True)\n",
    "        companyNumber.append(number)\n",
    "        status.append(tableContent[indexNumber+2].find(text=True))\n",
    "        link.append(\"https://find-and-update.company-information.service.gov.uk/company/\"+str(number)) \n",
    "\n",
    "    sub_df = pd.DataFrame({'companyName': companyName, 'companyNumber': companyNumber,'status': status,'link': link })\n",
    "    \n",
    "    return sub_df,urltext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b34135",
   "metadata": {},
   "outputs": [],
   "source": [
    "def s3_save_file_func(**kwargs):\n",
    "\n",
    "    import pandas as pd\n",
    "    import io\n",
    "\n",
    "    bucket_name = kwargs['de166-data']\n",
    "    key = kwargs['s3_key']\n",
    "    s3 = S3Hook(kwargs['aws_conn_id'])\n",
    "\n",
    "    # Get the task instance\n",
    "    task_instance = kwargs['ti']\n",
    "\n",
    "    # Get the output of the bash task\n",
    "    scraped_data_previous_task = task_instance.xcom_pull(task_ids=\"web_scraping_task\")\n",
    "\n",
    "    log.info('xcom from web_scraping_task:{0}'.format(scraped_data_previous_task))\n",
    "\n",
    "    # Load the list of dictionaries with the scraped data from the previous task into a pandas dataframe\n",
    "    log.info('Loading scraped data into pandas dataframe')\n",
    "    df = pd.DataFrame.from_dict(scraped_data_previous_task)\n",
    "\n",
    "    log.info('Saving scraped data to {0}'.format(key))\n",
    "\n",
    "    # Prepare the file to send to s3\n",
    "    csv_buffer = io.StringIO()\n",
    "    df.to_csv(csv_buffer, index=False)\n",
    "\n",
    "    # Save the pandas dataframe as a csv to s3\n",
    "    s3 = s3.get_resource_type('s3')\n",
    "\n",
    "    # Get the data type object from pandas dataframe, key and connection object to s3 bucket\n",
    "    data = csv_buffer.getvalue()\n",
    "\n",
    "    print(\"Saving CSV file\")\n",
    "    object = s3.Object(bucket_name, key)\n",
    "\n",
    "    # Write the file to S3 bucket in specific path defined in key\n",
    "    object.put(Body=data)\n",
    "\n",
    "    log.info('Finished saving the scraped data to s3')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf5ab90",
   "metadata": {},
   "outputs": [],
   "source": [
    "web_scraping_task = PythonOperator(\n",
    "    task_id='web_scraping_task',\n",
    "    provide_context=True,\n",
    "    python_callable=web_scraping_function,\n",
    "    op_kwargs=default_args,\n",
    "    dag=dag,\n",
    "\n",
    ")\n",
    "\n",
    "save_scraped_data_to_s3_task = PythonOperator(\n",
    "    task_id='save_scraped_data_to_s3_task',\n",
    "    provide_context=True,\n",
    "    python_callable=s3_save_file_func,\n",
    "    trigger_rule=TriggerRule.ALL_SUCCESS,\n",
    "    op_kwargs=default_args,\n",
    "    dag=dag,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b955977",
   "metadata": {},
   "outputs": [],
   "source": [
    "web_scraping_task >> save_scraped_data_to_s3_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aec5c1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
