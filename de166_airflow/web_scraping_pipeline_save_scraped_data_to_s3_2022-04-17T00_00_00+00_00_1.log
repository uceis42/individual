*** Reading local file: /usr/local/airflow/logs/web_scraping_pipeline/save_scraped_data_to_s3/2022-04-24T10:41:27.643572+00:00/1.log
[2022-04-24 10:41:34,559] {{taskinstance.py:655}} INFO - Dependencies all met for <TaskInstance: web_scraping_pipeline.save_scraped_data_to_s3 2022-04-24T10:41:27.643572+00:00 [queued]>
[2022-04-24 10:41:34,570] {{taskinstance.py:655}} INFO - Dependencies all met for <TaskInstance: web_scraping_pipeline.save_scraped_data_to_s3 2022-04-24T10:41:27.643572+00:00 [queued]>
[2022-04-24 10:41:34,571] {{taskinstance.py:866}} INFO - 
--------------------------------------------------------------------------------
[2022-04-24 10:41:34,571] {{taskinstance.py:867}} INFO - Starting attempt 1 of 1
[2022-04-24 10:41:34,571] {{taskinstance.py:868}} INFO - 
--------------------------------------------------------------------------------
[2022-04-24 10:41:34,577] {{taskinstance.py:887}} INFO - Executing <Task(PythonOperator): save_scraped_data_to_s3> on 2022-04-24T10:41:27.643572+00:00
[2022-04-24 10:41:34,579] {{standard_task_runner.py:53}} INFO - Started process 2713 to run task
[2022-04-24 10:41:34,612] {{logging_mixin.py:112}} INFO - Running %s on host %s <TaskInstance: web_scraping_pipeline.save_scraped_data_to_s3 2022-04-24T10:41:27.643572+00:00 [running]> 34cbaf6db028
[2022-04-24 10:41:34,784] {{logging_mixin.py:112}} INFO - [2022-04-24 10:41:34,784] {{de166dag.py:100}} INFO - xcom from web_scraping_task:None
[2022-04-24 10:41:34,784] {{logging_mixin.py:112}} INFO - [2022-04-24 10:41:34,784] {{de166dag.py:103}} INFO - Loading scraped data into pandas dataframe
[2022-04-24 10:41:34,784] {{logging_mixin.py:112}} INFO - [2022-04-24 10:41:34,784] {{de166dag.py:106}} INFO - Saving scraped data to output/DE166_web_scraping_pipeline/output.csv
[2022-04-24 10:41:34,790] {{taskinstance.py:1128}} ERROR - 
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 966, in _run_raw_task
    result = task_copy.execute(context=context)
  File "/usr/local/lib/python3.7/site-packages/airflow/operators/python_operator.py", line 113, in execute
    return_value = self.execute_callable()
  File "/usr/local/lib/python3.7/site-packages/airflow/operators/python_operator.py", line 118, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/usr/local/airflow/dags/de166dag.py", line 113, in s3_save_file_func
    s3 = s3.get_resource_type('s3')
  File "/usr/local/lib/python3.7/site-packages/airflow/contrib/hooks/aws_hook.py", line 183, in get_resource_type
    session, endpoint_url = self._get_credentials(region_name)
  File "/usr/local/lib/python3.7/site-packages/airflow/contrib/hooks/aws_hook.py", line 103, in _get_credentials
    extra_config = connection_object.extra_dejson
  File "/usr/local/lib/python3.7/site-packages/airflow/models/connection.py", line 338, in extra_dejson
    if self.extra:
  File "/usr/local/lib/python3.7/site-packages/sqlalchemy/orm/attributes.py", line 353, in __get__
    retval = self.descriptor.__get__(instance, owner)
  File "/usr/local/lib/python3.7/site-packages/airflow/models/connection.py", line 212, in get_extra
    return fernet.decrypt(bytes(self._extra, 'utf-8')).decode()
  File "/usr/local/lib/python3.7/site-packages/cryptography/fernet.py", line 171, in decrypt
    raise InvalidToken
cryptography.fernet.InvalidToken
[2022-04-24 10:41:34,793] {{taskinstance.py:1185}} INFO - Marking task as FAILED.dag_id=web_scraping_pipeline, task_id=save_scraped_data_to_s3, execution_date=20220424T104127, start_date=20220424T104134, end_date=20220424T104134

